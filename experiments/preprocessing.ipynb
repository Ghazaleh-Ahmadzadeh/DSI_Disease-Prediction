{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully from: ../data/raw/ObesityDataSet_raw_and_data_sinthetic.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Load Data and Perform Initial Preprocessing \n",
    "input_csv_path = os.path.join('..', 'data', 'raw', 'ObesityDataSet_raw_and_data_sinthetic.csv')\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    print(f\"Dataset loaded successfully from: {input_csv_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The file was not found at '{input_csv_path}'\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocessing \n",
    "# Use Ordinal Encoding for Ordered Features\n",
    "caec_mapping = {'no': 0, 'Sometimes': 1, 'Frequently': 2, 'Always': 3}\n",
    "calc_mapping = {'no': 0, 'Sometimes': 1, 'Frequently': 2, 'Always': 3} \n",
    "\n",
    "df_processed['CAEC'] = df_processed['CAEC'].map(caec_mapping)\n",
    "df_processed['CALC'] = df_processed['CALC'].map(calc_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age Categorization\n",
    "bins = [0, 18, 35, 55, df['Age'].max()]\n",
    "labels = ['Adolescent', 'Young Adult', 'Adult', 'Senior']\n",
    "df_processed['Age_Category'] = pd.cut(df_processed['Age'], bins=bins, labels=labels, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for the target variable 'NObeyesdad'\n",
    "le = LabelEncoder()\n",
    "df_processed['NObeyesdad'] = le.fit_transform(df_processed['NObeyesdad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding binary categorical features\n",
    "binary_features = ['family_history_with_overweight', 'FAVC', 'SMOKE', 'SCC']\n",
    "for feature in binary_features:\n",
    "    df_processed[feature] = df_processed[feature].map({'yes': 1, 'no': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial encoding and feature engineering complete.\n",
      "Preprocessed data shape: (2111, 22)\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encode remaining nominal features\n",
    "# Exclude 'CAEC' and 'CALC' as they are now ordinally encoded.\n",
    "categorical_features = ['Gender', 'MTRANS', 'Age_Category']\n",
    "df_processed = pd.get_dummies(df_processed, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Drop original columns that have been engineered or replaced\n",
    "df_processed.drop(['Age'], axis=1, inplace=True)\n",
    "print(\"\\nInitial encoding and feature engineering complete.\")\n",
    "print(\"Preprocessed data shape:\", df_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Split Data into Features (X) and Target (y) \n",
    "X = df_processed.drop('NObeyesdad', axis=1)\n",
    "y = df_processed['NObeyesdad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data split into training (1688 rows) and testing (423 rows) sets.\n"
     ]
    }
   ],
   "source": [
    "# 4. Split Data into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"\\nData split into training ({len(X_train)} rows) and testing ({len(X_test)} rows) sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory for output files: '../data/preprocessed/preprocessed_data_20250720_131842/'\n"
     ]
    }
   ],
   "source": [
    "# 5. Create a Unique Directory for Output Files\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = os.path.join('..', 'data', 'preprocessed', f\"preprocessed_data_{timestamp}\")\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Created directory for output files: '{output_dir}/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved baseline (unscaled) training and testing sets.\n"
     ]
    }
   ],
   "source": [
    "# 6. Save the Baseline (Unscaled) Datasets \n",
    "\n",
    "X_train.to_csv(os.path.join(output_dir, 'X_train_baseline.csv'), index=False)\n",
    "X_test.to_csv(os.path.join(output_dir, 'X_test_baseline.csv'), index=False)\n",
    "y_train.to_csv(os.path.join(output_dir, 'y_train_baseline.csv'), index=False)\n",
    "y_test.to_csv(os.path.join(output_dir, 'y_test_baseline.csv'), index=False)\n",
    "print(\"\\nSaved baseline (unscaled) training and testing sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using scaler: MinMaxScaler\n",
      "Scaling applied successfully.\n"
     ]
    }
   ],
   "source": [
    "# 7. Apply Scaling\n",
    "\n",
    "# MinMaxScaler \n",
    "scaler = MinMaxScaler(feature_range=(1, 5))\n",
    "\n",
    "print(f\"\\nUsing scaler: {type(scaler).__name__}\")\n",
    "\n",
    "# Identify numerical features to scale. We now include 'BMI'.\n",
    "numerical_features = ['Weight', 'Height', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE', 'CAEC', 'CALC']\n",
    "\n",
    "# Create copies to avoid changing the original baseline dataframes\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Fit the scaler ONLY on the training data\n",
    "scaler.fit(X_train_scaled[numerical_features])\n",
    "\n",
    "# Transform both the training and testing data\n",
    "X_train_scaled[numerical_features] = scaler.transform(X_train_scaled[numerical_features])\n",
    "X_test_scaled[numerical_features] = scaler.transform(X_test_scaled[numerical_features])\n",
    "\n",
    "print(\"Scaling applied successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final scaled training and testing sets.\n",
      "\n",
      "All files saved in: ../data/preprocessed/preprocessed_data_20250720_131842\n"
     ]
    }
   ],
   "source": [
    "# 8. Save the Final Scaled Datasets\n",
    "\n",
    "X_train_scaled.to_csv(os.path.join(output_dir, 'X_train_scaled.csv'), index=False)\n",
    "X_test_scaled.to_csv(os.path.join(output_dir, 'X_test_scaled.csv'), index=False)\n",
    "y_train.to_csv(os.path.join(output_dir, 'y_train_scaled.csv'), index=False)\n",
    "y_test.to_csv(os.path.join(output_dir, 'y_test_scaled.csv'), index=False)\n",
    "print(\"Saved final scaled training and testing sets.\")\n",
    "print(f\"\\nAll files saved in: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
